{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_collection.parse_pcap import pcap_to_pandas\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "**Start downloading this file and place it in AI4ALL-IoT/example_pcaps: https://drive.google.com/file/d/1Lr1dleCbZcQWfHoW_u6Q2uZFte17Y2Z_/view?usp=sharing. You'll need it later!**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Today, we're going to explore probability. The concept of probability is a powerful tool that lets us answer interesting questions about our data, and it serves as the foundation of a commonly used machine learning technique for classification We'll also be building a Naive Bayes classifier from scratch, so you'll get hands-on experience coding a machine learning classifier from scratch!\n",
    "\n",
    "Let's start with some simple probability examples on the board. Let's see how much you can recall from lecture!\n",
    "\n",
    "Say I have a bucket with 10 blue balls and 20 red balls. If I choose a ball at random from the bucket, what is the probability that I choose a red ball? That is, we want to calculate:\n",
    "\n",
    "$P($red ball$)\\ =\\ ??$\n",
    "\n",
    "This is equal to the fraction of red balls over the total number of balls.\n",
    "\n",
    "$P($red ball$)\\ =\\ \\frac{\\text{# of red balls}}{\\text{# of total balls}}\\ =\\ \\frac{20}{30}\\ =\\ \\frac{2}{3}$\n",
    "\n",
    "Similarly, the chance of picking a blue ball is:\n",
    "\n",
    "$P($blue ball$)\\ =\\ \\frac{\\text{# of blue balls}}{\\text{# of total balls}}\\ =\\ \\frac{10}{30}\\ =\\ \\frac{1}{3}$\n",
    "\n",
    "Now, let's say we want to find the probability of picking a red ball out of the bucket, **AND THEN** picking a blue ball out of the bucket. When we want to find the probability of two events both occurring, we multiply their probabilities together. The resulting probability is:\n",
    "\n",
    "$P(\\text{red ball})*P(\\text{blue ball}\\ |\\ \\text{red ball missing})$\n",
    "\n",
    "Here, we introduce the concept of conditional probability. $P(\\text{blue ball}\\ |\\ \\text{red ball missing})$ represents the probability that a blue ball is pulled from the bucket, **given** that a red ball has already been taken out.\n",
    "\n",
    "Are these two events independent? Does pulling a red ball affect the result of the probability of pulling a red ball followed by a blue ball? If it had no effect, the overall probability would be equivalent to:\n",
    "\n",
    "$P(\\text{red ball})*P(\\text{blue ball})$\n",
    "\n",
    "But it's not! By removing a red ball, there are now fewer overall balls to choose from, which changes the resulting probability. The full probability is therefore calculated as:\n",
    "\n",
    "$P(\\text{red ball})*P(\\text{blue ball}\\ |\\ \\text{red ball missing})\\ =\\ \\frac{20}{30}*\\frac{10}{29}\\ =\\ \\frac{20}{87}$\n",
    "\n",
    "Now that you've had a chance to review, let's dive into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Probability of a TCP Packet\n",
    "\n",
    "Let's compute the probability that a packet from our capture was a TCP packet:\n",
    "\n",
    "$P(\\text{TCP Packet})\\ =\\ \\frac{\\text{# of TCP packets}}{\\text{# of total packets}}$\n",
    "\n",
    "We'll start by loading some captured data into Python, and filtering out packets that don't have a DNS query field or a DNS response field. You'll need to fill in the blanks with the correct information. For tcp_packets, there are three options for each blank: \"data\", \"protocol\", or \"TCP\". Consult yesterday's lab if you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ?? # call our helper \"pcap_to_pandas\" function, and pass in the argument \"example_pcaps/tplink_switch.pcap\"\n",
    "tcp_packets = ??[??[??] == \"??\"] # packets with the protocol column equal to \"TCP\"\n",
    "\n",
    "# len gives the number of packets in some data\n",
    "num_tcp_packets = len(??) # number of TCP packets\n",
    "num_total_packets = len(??) # number of total packets\n",
    "\n",
    "tcp_probability = ?? / ?? # probability that a packet is a TCP packet\n",
    "\n",
    "print(tcp_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of a DNS Packet, Given Source Port or Dest Port is 53\n",
    "\n",
    "Now, let's compute the probability that a packet from our capture was a DNS packet, given that at least one of its ports was 53. We define a DNS packet as a packet that has a DNS query **OR** a DNS response field. We are calculating:\n",
    "\n",
    "$P($DNS Query $\\cup$ DNS Response | Source Port == 53 $\\cup$ Dst Port == 53$)$\n",
    "\n",
    "The $\\cup$ means \"OR\".\n",
    "\n",
    "The probability can be calculated as:\n",
    "\n",
    "$P(\\text{DNS Query} \\cup \\text{DNS Response}\\ |\\ \\text{Source Port == 53} \\cup \\text{Dst Port == 53})\\ =\\ \\frac{\\text{# of packets with a DNS query or DNS response field}}{\\text{# of packets with a SRC port or DST port 53}}$\n",
    "\n",
    "Because of conditional probability, rather than dividing by the total number of packets, we divide by only the # of packets that satisfy the condition that the SRC or DST port is equal to 53.\n",
    "\n",
    "You'll need to fill in the blanks with the correct information. For dns_queries and dns_responses, there are three options for each blank: \"data\", \"dns_query\", or \"dns_resp\". Consult yesterday's lab if you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dns_queries = data[data[\"dns_query\"].notnull()] # packets with a DNS query column that isn't None\n",
    "dns_responses = data[data[\"dns_resp\"].notnull()] # packets with a DNS response column that isn't None\n",
    "\n",
    "src_port_53 = data[data[\"port_src\"] == 53]\n",
    "dst_port_53 = data[data[\"port_dst\"] == 53]\n",
    "\n",
    "num_dns_queries = len(dns_queries)\n",
    "num_dns_responses = len(dns_responses)\n",
    "num_dns_total = num_dns_queries + num_dns_responses\n",
    "\n",
    "num_port_53 = len(src_port_53) + len(dst_port_53)\n",
    "\n",
    "# Note: This is tricky! Consult the DNS columns of the data in this notebook and/or Wireshark. if you are stuck.\n",
    "dns_probability = num_dns_queries / num_port_53 # probability that a packet is a DNS packet, given that at least one port is 53\n",
    "\n",
    "print(dns_probability) # Should be 1 (100%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect an answer of 100%. If you got over 100% instead, your probability is likely overcounting some packets!\n",
    "\n",
    "Hint: Examine the \"dns_query\" and \"dns_resp\" columns of packets that contain a DNS query or response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability that a DNS Response is Longer than the Mean Packet Length\n",
    "\n",
    "Now let's answer the following questions about our packets. What is the probability that a given DNS response has a length longer than the average length of all packets?\n",
    "\n",
    "$P($Length > Mean Length of **All** Packets | DNS Response$)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mean_length = ??[??].mean() # the mean length of all packets\n",
    "longer_than_mean = dns_responses[dns_responses[\"length\"] > ??] # number of DNS packets with a length longer than mean_length\n",
    "\n",
    "num_longer = len(longer_than_mean)\n",
    "print(num_longer / num_dns_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly half! Let's open Wireshark and examine these packets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises\n",
    "\n",
    "1. (Challenge!) Find the probability that a DNS request is immediately followed by a DNS response in the packet trace. This will give us an idea of how fast DNS responses are received, relative to other network traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Now we're going to use the Naive Bayes algorithm to predict which task a user is most likely doing given a particular packet. While there are existing python functions for performing a naive Bayes classification, we already know everything we need to do it ourselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous lecture, we first need to label the data with what activity was happening at the time each packet is received.\n",
    "\n",
    "First, download the ross.pcap file at https://drive.google.com/file/d/1Lr1dleCbZcQWfHoW_u6Q2uZFte17Y2Z_/view?usp=sharing. Place it in the AI4ALL-IoT/example_pcaps folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, may take a few minutes.\n",
    "data = pcap_to_pandas(\"example_pcaps/ross.pcap\")\n",
    "labels = pd.read_csv('example_pcaps/ross_labels.txt', header=None, names=[\"time\", \"activity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's add the timestamp (remember, this is measured in seconds since the epoch) to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(time):\n",
    "    return datetime.strptime(time, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "labels['datetime'] = labels['time'].apply(convert_to_datetime)\n",
    "\n",
    "tzlocal = datetime.now().astimezone().tzinfo\n",
    "labels['timestamp'] = labels['datetime'].apply(lambda dt: dt.replace(tzinfo=tzlocal).timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to use the activity log to label the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels['label'] = label_encoder.fit_transform(labels['activity'])\n",
    "\n",
    "for index, row in labels.iterrows():\n",
    "    data.loc[data['time'] >= row['timestamp'], 'label'] = row['label']\n",
    "    \n",
    "num_labels = max(labels['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're going to take 20% of the data set and reserve it as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "test = data[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The simplest statistic we need to compute is the probability that each label occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_probs = np.zeros(num_labels + 1)\n",
    "\n",
    "for i in range(num_labels + 1):\n",
    "    label_probs[i] = len(train[train['label'] == i]) / len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to go through the training set and tally up what values appear in different fields and how often they appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_table = {} # The table stores tuples of the form (label, value)\n",
    "ip_list = [] # The list stores every value we've seen (uniquely).\n",
    "\n",
    "dns_table = {}\n",
    "dns_list = []\n",
    "\n",
    "port_table = {}\n",
    "port_list = []\n",
    "\n",
    "protocol_table = {}\n",
    "protocol_list = []\n",
    "\n",
    "def update_table(table, lst, label, value):\n",
    "    if value is not None:\n",
    "        table[(label, value)] = table.get((label, value), 0) + 1\n",
    "    if value is not None and value not in lst:\n",
    "        lst.append(value)\n",
    "\n",
    "for index, row in train.iterrows():\n",
    "    update_table(ip_table, ip_list, row['label'], row['ip_src'])\n",
    "    update_table(ip_table, ip_list, row['label'], row['ip_dst'])\n",
    "    update_table(port_table, port_list, row['label'], row['port_src'])\n",
    "    update_table(port_table, port_list, row['label'], row['port_dst'])\n",
    "    update_table(protocol_table, protocol_list, row['label'], row['protocol'])\n",
    "    update_table(dns_table, dns_list, row['label'], row['dns_query'])\n",
    "    update_table(dns_table, dns_list, row['label'], row['dns_resp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use these tallies to compute the logarithm of the event probabilites. Typically, we prefer to work with log probabilities because many of these events have very small chances of happening. When multiplied together the resulting joint probability often end up inconveniently small for computers to work with. Taking logarithms will not change what we are trying to do conceptually, but improves the numerical properties of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(table, lst, smoothing):\n",
    "    log_probs = {}\n",
    "    \n",
    "    for l in range(num_labels + 1):\n",
    "        total = sum([table.get((l, val), 0) for val in lst])\n",
    "        \n",
    "        for val in lst:\n",
    "            if (l, val) in table:\n",
    "                log_probs[(l, val)] = log(table[(l, val)] + smoothing) - log(total + smoothing * (len(lst) + 1))\n",
    "        \n",
    "        log_probs[(l, '<UNK>')] = log(smoothing) - log(total + smoothing * (len(lst) + 1))\n",
    "    \n",
    "    return log_probs\n",
    "\n",
    "ip_log_prob = compute_log_probs(ip_table, ip_list, 1e-5)\n",
    "dns_log_prob = compute_log_probs(dns_table, dns_list, 1e-5)\n",
    "port_log_prob = compute_log_probs(port_table, port_list, 1e-5)\n",
    "protocol_log_prob = compute_log_probs(protocol_table, protocol_list, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to create the classifier. When presented with a new row of data, we simply sum all the relevant log probabilities for each class and report the class with the highest log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5682810061628799\n"
     ]
    }
   ],
   "source": [
    "def get_log_prob(table, val, label):\n",
    "    return table.get((label, val), table[(label, '<UNK>')])\n",
    "\n",
    "def classify(row):\n",
    "    best_label = -1\n",
    "    best_label_score = float('-Inf')\n",
    "    \n",
    "    for l in range(num_labels + 1):\n",
    "        score = log(label_probs[l])\n",
    "        \n",
    "        score = score + get_log_prob(ip_log_prob, row['ip_src'], l)\n",
    "        score = score + get_log_prob(ip_log_prob, row['ip_dst'], l)\n",
    "        \n",
    "        if row['is_dns']:\n",
    "            if row['dns_query'] is not None:\n",
    "                score = score + get_log_prob(dns_log_prob, row['dns_query'], l)\n",
    "            \n",
    "            if row['dns_resp'] is not None:\n",
    "                score = score + get_log_prob(dns_log_prob, row['dns_resp'], l)\n",
    "        \n",
    "        score = score + get_log_prob(port_log_prob, row['port_src'], l)\n",
    "        score = score + get_log_prob(port_log_prob, row['port_dst'], l)\n",
    "        \n",
    "        score = score + get_log_prob(protocol_log_prob, row['protocol'], l)\n",
    "        \n",
    "        if score > best_label_score:\n",
    "            best_label = l\n",
    "            best_label_score = score\n",
    "    \n",
    "    return best_label\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    if classify(row) == row['label']:\n",
    "        correct = correct + 1\n",
    "\n",
    "print('Accuracy: {}'.format(correct / len(test)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
